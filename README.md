# Understanding Toy Backdoors via Mechanistic Interpretability
**Abstract:** Backdoors and hidden harmful behaviour represent a severe risk to the safe deployment of deep neural networks. In this paper, we explore how a small Transformer model implements a toy backdoor behaviour. Our head attribution and activation patching experiments suggest that our model uses a single attention head to implement a simple backdoor. *Colab notebooks for the experiments are available in the Google Drive Folder.*

Link to Paper: https://drive.google.com/file/d/1z_Cyf4Od9oleou-eSpmaR3_GqAMdl3Xk/view?usp=sharing <br>
Link to Drive Folder: https://drive.google.com/drive/folders/1bYo5cmAbYJfbly7my4PkCFa21mJxQtdu?usp=sharing
Link to Course Page: https://www.cl.cam.ac.uk/teaching/2324/R252/ <br>
